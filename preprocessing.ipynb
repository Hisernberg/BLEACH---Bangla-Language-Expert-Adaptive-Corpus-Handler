{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14014789,"sourceType":"datasetVersion","datasetId":8928180},{"sourceId":14014838,"sourceType":"datasetVersion","datasetId":8928207},{"sourceId":14015072,"sourceType":"datasetVersion","datasetId":8928358}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set style for better visualizations\nsns.set_style(\"whitegrid\")\nplt.rcParams['figure.figsize'] = (12, 6)\nplt.rcParams['font.size'] = 10\n\n# ============================================================================\n# CONFIGURATION\n# ============================================================================\n\n# Define the 5 dialects we're working with\nDIALECTS = ['Chittagong', 'Sylhet', 'Barisal', 'Noakhali', 'Mymensingh']\n\n# Vashantor uses different spelling\nDIALECT_MAPPING = {\n    'Chittagong': 'Chittagong',\n    'Sylhet': 'Sylhet',\n    'Barisal': ['Barisal', 'Barishal'],  # Handle both spellings\n    'Noakhali': 'Noakhali',\n    'Mymensingh': 'Mymensingh'\n}\n\n# Kaggle dataset paths\nBANGLADIAL_PATH = '/kaggle/input/bangladiel/BanglaDial A Merged and Imbalanced text Dataset for Bengali Regional dialect analysis. (1).csv'\nVASHANTOR_BASE_PATH = '/kaggle/input/vashantor010'\n\n# Output paths\nOUTPUT_DIR = '/kaggle/working/'\n\n# ============================================================================\n# HELPER FUNCTIONS\n# ============================================================================\n\ndef clean_text(text):\n    \"\"\"\n    Clean and normalize text with:\n    - Emoji removal\n    - Hashtag/mention removal\n    - Whitespace normalization\n    - Punctuation spacing standardization\n    - Lowercasing\n    \"\"\"\n    if pd.isna(text) or not isinstance(text, str):\n        return \"\"\n    \n    # Remove emojis (broad Unicode range coverage)\n    emoji_pattern = re.compile(\n        \"[\"\n        \"\\U0001F600-\\U0001F64F\"  # emoticons\n        \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n        \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n        \"\\U0001F1E0-\\U0001F1FF\"  # flags\n        \"\\U00002702-\\U000027B0\"\n        \"\\U000024C2-\\U0001F251\"\n        \"\\U0001F900-\\U0001F9FF\"  # supplemental symbols\n        \"\\U0001FA70-\\U0001FAFF\"\n        \"]+\", flags=re.UNICODE\n    )\n    text = emoji_pattern.sub(r'', text)\n    \n    # Remove hashtags and mentions\n    text = re.sub(r'#\\w+', '', text)\n    text = re.sub(r'@\\w+', '', text)\n    \n    # Remove URLs\n    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n    \n    # Normalize whitespace\n    text = re.sub(r'\\s+', ' ', text)\n    \n    # Standardize punctuation spacing (add space after punctuation)\n    text = re.sub(r'([।!?,.;:])([^\\s])', r'\\1 \\2', text)\n    \n    # Remove extra spaces before punctuation\n    text = re.sub(r'\\s+([।!?,.])', r'\\1', text)\n    \n    # Lowercase\n    text = text.lower()\n    \n    # Strip leading/trailing whitespace\n    text = text.strip()\n    \n    return text\n\n\ndef is_english_only(text):\n    \"\"\"\n    Check if text contains only English characters (no Bangla script).\n    Returns True if text is English-only or empty.\n    \"\"\"\n    if not text or len(text.strip()) == 0:\n        return True\n    \n    # Check for Bangla Unicode range (0980-09FF)\n    bangla_chars = re.findall(r'[\\u0980-\\u09FF]', text)\n    \n    # If no Bangla characters found, it's English-only\n    return len(bangla_chars) == 0\n\n\ndef normalize_bangla(text):\n    \"\"\"\n    Apply minimal normalization for Bangla script:\n    - Normalize certain variations\n    - Remove zero-width characters\n    \"\"\"\n    # Remove zero-width characters\n    text = re.sub(r'[\\u200B-\\u200D\\uFEFF]', '', text)\n    \n    return text\n\n\ndef plot_dialect_distribution(df, title, filename):\n    \"\"\"Create bar plot for dialect distribution\"\"\"\n    fig, ax = plt.subplots(figsize=(10, 6))\n    counts = df['dialect'].value_counts()\n    \n    colors = sns.color_palette(\"husl\", len(counts))\n    bars = ax.bar(counts.index, counts.values, color=colors, edgecolor='black', linewidth=1.2)\n    \n    # Add value labels on bars\n    for bar in bars:\n        height = bar.get_height()\n        ax.text(bar.get_x() + bar.get_width()/2., height,\n                f'{int(height)}',\n                ha='center', va='bottom', fontsize=11, fontweight='bold')\n    \n    ax.set_xlabel('Dialect', fontsize=12, fontweight='bold')\n    ax.set_ylabel('Number of Samples', fontsize=12, fontweight='bold')\n    ax.set_title(title, fontsize=14, fontweight='bold', pad=20)\n    plt.xticks(rotation=45, ha='right')\n    plt.tight_layout()\n    plt.savefig(OUTPUT_DIR + filename, dpi=300, bbox_inches='tight')\n    plt.show()\n    print(f\"✓ Saved: {OUTPUT_DIR}{filename}\")\n\n\ndef plot_text_length_distribution(df, title, filename):\n    \"\"\"Create histogram and box plot for text length distribution\"\"\"\n    df['text_length'] = df['text'].str.len()\n    \n    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n    \n    # Histogram\n    axes[0].hist(df['text_length'], bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n    axes[0].set_xlabel('Text Length (characters)', fontsize=11, fontweight='bold')\n    axes[0].set_ylabel('Frequency', fontsize=11, fontweight='bold')\n    axes[0].set_title('Text Length Distribution', fontsize=12, fontweight='bold')\n    axes[0].axvline(df['text_length'].mean(), color='red', linestyle='--', \n                    linewidth=2, label=f'Mean: {df[\"text_length\"].mean():.0f}')\n    axes[0].legend()\n    \n    # Box plot by dialect\n    dialect_order = df['dialect'].value_counts().index.tolist()\n    sns.boxplot(data=df, y='dialect', x='text_length', order=dialect_order, \n                palette='Set2', ax=axes[1])\n    axes[1].set_xlabel('Text Length (characters)', fontsize=11, fontweight='bold')\n    axes[1].set_ylabel('Dialect', fontsize=11, fontweight='bold')\n    axes[1].set_title('Text Length by Dialect', fontsize=12, fontweight='bold')\n    \n    plt.suptitle(title, fontsize=14, fontweight='bold', y=1.02)\n    plt.tight_layout()\n    plt.savefig(OUTPUT_DIR + filename, dpi=300, bbox_inches='tight')\n    plt.show()\n    print(f\"✓ Saved: {OUTPUT_DIR}{filename}\")\n\n\ndef plot_split_comparison(train_df, val_df, test_df, filename):\n    \"\"\"Create comparison plot for train/val/test splits\"\"\"\n    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n    \n    splits = [\n        (train_df, 'Training Set', axes[0]),\n        (val_df, 'Validation Set', axes[1]),\n        (test_df, 'Test Set', axes[2])\n    ]\n    \n    for df, title, ax in splits:\n        counts = df['dialect'].value_counts()\n        colors = sns.color_palette(\"husl\", len(counts))\n        bars = ax.bar(counts.index, counts.values, color=colors, \n                      edgecolor='black', linewidth=1.2)\n        \n        for bar in bars:\n            height = bar.get_height()\n            ax.text(bar.get_x() + bar.get_width()/2., height,\n                    f'{int(height)}',\n                    ha='center', va='bottom', fontsize=9, fontweight='bold')\n        \n        ax.set_xlabel('Dialect', fontsize=10, fontweight='bold')\n        ax.set_ylabel('Number of Samples', fontsize=10, fontweight='bold')\n        ax.set_title(f'{title}\\n(Total: {len(df)})', fontsize=11, fontweight='bold')\n        ax.tick_params(axis='x', rotation=45)\n        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n    \n    plt.suptitle('Train/Validation/Test Split Distribution', fontsize=14, fontweight='bold')\n    plt.tight_layout()\n    plt.savefig(OUTPUT_DIR + filename, dpi=300, bbox_inches='tight')\n    plt.show()\n    print(f\"✓ Saved: {OUTPUT_DIR}{filename}\")\n\n\ndef plot_data_source_comparison(bangladial_df, vashantor_df, filename):\n    \"\"\"Compare data sources\"\"\"\n    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n    \n    # Source 1: BanglaDial\n    counts1 = bangladial_df['dialect'].value_counts()\n    colors1 = sns.color_palette(\"husl\", len(counts1))\n    bars1 = axes[0].bar(counts1.index, counts1.values, color=colors1, \n                        edgecolor='black', linewidth=1.2)\n    \n    for bar in bars1:\n        height = bar.get_height()\n        axes[0].text(bar.get_x() + bar.get_width()/2., height,\n                     f'{int(height)}',\n                     ha='center', va='bottom', fontsize=10, fontweight='bold')\n    \n    axes[0].set_xlabel('Dialect', fontsize=11, fontweight='bold')\n    axes[0].set_ylabel('Number of Samples', fontsize=11, fontweight='bold')\n    axes[0].set_title(f'BanglaDial Dataset\\n(Total: {len(bangladial_df)})', \n                      fontsize=12, fontweight='bold')\n    axes[0].tick_params(axis='x', rotation=45)\n    plt.setp(axes[0].xaxis.get_majorticklabels(), rotation=45, ha='right')\n    \n    # Source 2: Vashantor\n    counts2 = vashantor_df['dialect'].value_counts()\n    colors2 = sns.color_palette(\"husl\", len(counts2))\n    bars2 = axes[1].bar(counts2.index, counts2.values, color=colors2, \n                        edgecolor='black', linewidth=1.2)\n    \n    for bar in bars2:\n        height = bar.get_height()\n        axes[1].text(bar.get_x() + bar.get_width()/2., height,\n                     f'{int(height)}',\n                     ha='center', va='bottom', fontsize=10, fontweight='bold')\n    \n    axes[1].set_xlabel('Dialect', fontsize=11, fontweight='bold')\n    axes[1].set_ylabel('Number of Samples', fontsize=11, fontweight='bold')\n    axes[1].set_title(f'Vashantor Dataset\\n(Total: {len(vashantor_df)})', \n                      fontsize=12, fontweight='bold')\n    axes[1].tick_params(axis='x', rotation=45)\n    plt.setp(axes[1].xaxis.get_majorticklabels(), rotation=45, ha='right')\n    \n    plt.suptitle('Dataset Source Comparison', fontsize=14, fontweight='bold')\n    plt.tight_layout()\n    plt.savefig(OUTPUT_DIR + filename, dpi=300, bbox_inches='tight')\n    plt.show()\n    print(f\"✓ Saved: {OUTPUT_DIR}{filename}\")\n\n\n# ============================================================================\n# LOAD BANGLADIAL DATASET\n# ============================================================================\n\nprint(\"=\" * 80)\nprint(\"LOADING BANGLADIAL DATASET\")\nprint(\"=\" * 80)\n\n# Try different encodings if needed\ntry:\n    bangladial_df = pd.read_csv(BANGLADIAL_PATH, encoding='utf-8')\nexcept:\n    try:\n        bangladial_df = pd.read_csv(BANGLADIAL_PATH, encoding='latin-1')\n    except:\n        bangladial_df = pd.read_csv(BANGLADIAL_PATH, encoding='cp1252')\n\nprint(f\"Original BanglaDial shape: {bangladial_df.shape}\")\nprint(f\"Columns: {bangladial_df.columns.tolist()}\")\n\n# Detect text and dialect columns\ntext_col = None\ndialect_col = None\n\nfor col in bangladial_df.columns:\n    col_lower = col.lower()\n    if 'text' in col_lower or 'sentence' in col_lower or 'content' in col_lower:\n        text_col = col\n    if 'dialect' in col_lower or 'label' in col_lower or 'class' in col_lower or 'region' in col_lower:\n        dialect_col = col\n\nif text_col is None or dialect_col is None:\n    # Fallback to first two columns\n    text_col = bangladial_df.columns[0]\n    dialect_col = bangladial_df.columns[1]\n\nprint(f\"\\nUsing columns: text='{text_col}', dialect='{dialect_col}'\")\n\n# Rename columns for consistency\nbangladial_df = bangladial_df.rename(columns={text_col: 'text', dialect_col: 'dialect'})\n\n# Standardize dialect names (handle variations)\nbangladial_df['dialect'] = bangladial_df['dialect'].str.strip()\n\n# Filter for our 5 dialects (handle both spellings of Barisal)\nbangladial_df = bangladial_df[\n    (bangladial_df['dialect'] == 'Chittagong') |\n    (bangladial_df['dialect'] == 'Sylhet') |\n    (bangladial_df['dialect'].isin(['Barisal', 'Barishal'])) |\n    (bangladial_df['dialect'] == 'Noakhali') |\n    (bangladial_df['dialect'] == 'Mymensingh')\n].copy()\n\n# Standardize Barisal spelling\nbangladial_df.loc[bangladial_df['dialect'] == 'Barishal', 'dialect'] = 'Barisal'\n\nprint(f\"\\nFiltered BanglaDial shape (5 dialects): {bangladial_df.shape}\")\nprint(f\"Dialect distribution:\\n{bangladial_df['dialect'].value_counts()}\")\n\n# ============================================================================\n# LOAD VASHANTOR DATASET\n# ============================================================================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"LOADING VASHANTOR DATASET\")\nprint(\"=\" * 80)\n\nvashantor_dfs = []\n\nfor dialect in tqdm(DIALECTS, desc=\"Loading Vashantor files\"):\n    # Handle both Barisal and Barishal spellings\n    dialect_variants = [dialect]\n    if dialect == 'Barisal':\n        dialect_variants = ['Barisal', 'Barishal']\n    \n    for variant in dialect_variants:\n        for split in ['Train', 'Validation', 'Test']:\n            filename = f\"{variant} {split} Translation.csv\"\n            filepath = f\"{VASHANTOR_BASE_PATH}/{filename}\"\n            \n            try:\n                df = pd.read_csv(filepath, encoding='utf-8')\n                \n                # Identify columns with Bangla and Banglish text\n                bangla_cols = []\n                \n                # Look for region-specific Bangla columns (e.g., chittagong_bangla_speech)\n                for col in df.columns:\n                    col_lower = col.lower()\n                    # Include bangla_speech, banglish_speech, and region-specific columns\n                    if any(term in col_lower for term in ['bangla_speech', 'banglish_speech']):\n                        if 'english' not in col_lower:  # Exclude English columns\n                            bangla_cols.append(col)\n                \n                # If no specific columns found, use all except English\n                if not bangla_cols:\n                    bangla_cols = [col for col in df.columns \n                                  if 'english' not in col.lower()]\n                \n                # Extract text from Bangla/Banglish columns\n                for col in bangla_cols:\n                    temp_df = pd.DataFrame({\n                        'text': df[col],\n                        'dialect': 'Barisal' if variant == 'Barishal' else dialect\n                    })\n                    vashantor_dfs.append(temp_df)\n                \n                print(f\"✓ Loaded: {filename} ({len(df)} rows, {len(bangla_cols)} text columns)\")\n                break  # If successful, don't try other variants\n                \n            except FileNotFoundError:\n                continue  # Try next variant\n            except Exception as e:\n                print(f\"Error loading {filename}: {e}\")\n                continue\n\n# Combine all Vashantor data\nif vashantor_dfs:\n    vashantor_df = pd.concat(vashantor_dfs, ignore_index=True)\n    print(f\"\\nVashantor combined shape: {vashantor_df.shape}\")\n    print(f\"Dialect distribution:\\n{vashantor_df['dialect'].value_counts()}\")\nelse:\n    print(\"Warning: No Vashantor data loaded!\")\n    vashantor_df = pd.DataFrame(columns=['text', 'dialect'])\n\n# ============================================================================\n# VISUALIZE ORIGINAL DATASETS\n# ============================================================================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"VISUALIZING ORIGINAL DATASETS\")\nprint(\"=\" * 80)\n\nif len(bangladial_df) > 0 and len(vashantor_df) > 0:\n    plot_data_source_comparison(bangladial_df, vashantor_df, \n                                'viz_01_source_comparison.png')\n\n# ============================================================================\n# MERGE DATASETS\n# ============================================================================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"MERGING DATASETS\")\nprint(\"=\" * 80)\n\n# Combine BanglaDial and Vashantor\ncombined_df = pd.concat([bangladial_df[['text', 'dialect']], \n                         vashantor_df[['text', 'dialect']]], \n                        ignore_index=True)\n\nprint(f\"Combined dataset shape: {combined_df.shape}\")\nprint(f\"Dialect distribution:\\n{combined_df['dialect'].value_counts()}\")\n\n# Visualize combined dataset before cleaning\nplot_dialect_distribution(combined_df, \n                         'Combined Dataset (Before Cleaning)',\n                         'viz_02_combined_before_cleaning.png')\n\n# ============================================================================\n# DATA CLEANING\n# ============================================================================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"CLEANING DATA\")\nprint(\"=\" * 80)\n\n# Track cleaning steps\ncleaning_stats = []\n\ninitial_count = len(combined_df)\ncleaning_stats.append(('Initial', initial_count))\n\n# Drop rows with missing text\nprint(f\"Missing values before cleaning: {combined_df['text'].isna().sum()}\")\ncombined_df = combined_df.dropna(subset=['text'])\ncleaning_stats.append(('After dropping NaN', len(combined_df)))\n\n# Apply text cleaning\ntqdm.pandas(desc=\"Cleaning text\")\ncombined_df['text'] = combined_df['text'].progress_apply(clean_text)\n\n# Apply Bangla normalization\ntqdm.pandas(desc=\"Normalizing Bangla\")\ncombined_df['text'] = combined_df['text'].progress_apply(normalize_bangla)\n\n# Remove empty strings\ncombined_df = combined_df[combined_df['text'].str.len() > 0]\ncleaning_stats.append(('After removing empty', len(combined_df)))\n\n# Remove English-only sentences\nprint(f\"\\nRows before English removal: {len(combined_df)}\")\ntqdm.pandas(desc=\"Filtering English-only\")\ncombined_df = combined_df[~combined_df['text'].progress_apply(is_english_only)]\nprint(f\"Rows after English removal: {len(combined_df)}\")\ncleaning_stats.append(('After English removal', len(combined_df)))\n\n# Remove duplicates\nprint(f\"\\nRows before deduplication: {len(combined_df)}\")\ncombined_df = combined_df.drop_duplicates(subset=['text'], keep='first')\nprint(f\"Rows after deduplication: {len(combined_df)}\")\ncleaning_stats.append(('After deduplication', len(combined_df)))\n\n# Reset index\ncombined_df = combined_df.reset_index(drop=True)\n\nprint(f\"\\nFinal cleaned dataset shape: {combined_df.shape}\")\nprint(f\"Final dialect distribution:\\n{combined_df['dialect'].value_counts()}\")\n\n# Visualize cleaning impact\nfig, ax = plt.subplots(figsize=(10, 6))\nsteps = [s[0] for s in cleaning_stats]\ncounts = [s[1] for s in cleaning_stats]\ncolors = sns.color_palette(\"RdYlGn_r\", len(steps))\n\nbars = ax.barh(steps, counts, color=colors, edgecolor='black', linewidth=1.2)\n\nfor i, (bar, count) in enumerate(zip(bars, counts)):\n    width = bar.get_width()\n    ax.text(width, bar.get_y() + bar.get_height()/2.,\n            f' {count:,}',\n            ha='left', va='center', fontsize=11, fontweight='bold')\n\nax.set_xlabel('Number of Samples', fontsize=12, fontweight='bold')\nax.set_title('Data Cleaning Pipeline Impact', fontsize=14, fontweight='bold', pad=20)\nax.invert_yaxis()\nplt.tight_layout()\nplt.savefig(OUTPUT_DIR + 'viz_03_cleaning_pipeline.png', dpi=300, bbox_inches='tight')\nplt.show()\nprint(f\"✓ Saved: {OUTPUT_DIR}viz_03_cleaning_pipeline.png\")\n\n# Visualize cleaned dataset\nplot_dialect_distribution(combined_df, \n                         'Combined Dataset (After Cleaning)',\n                         'viz_04_combined_after_cleaning.png')\n\nplot_text_length_distribution(combined_df,\n                              'Text Length Analysis (Cleaned Dataset)',\n                              'viz_05_text_length_analysis.png')\n\n# ============================================================================\n# TRAIN/VAL/TEST SPLIT\n# ============================================================================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"SPLITTING DATA\")\nprint(\"=\" * 80)\n\n# Check if we have enough samples per dialect\nmin_samples = combined_df['dialect'].value_counts().min()\nprint(f\"Minimum samples per dialect: {min_samples}\")\n\nif min_samples < 10:\n    print(\"Warning: Some dialects have very few samples. Adjusting split strategy.\")\n    test_size = 0.15\n    val_size = 0.15\nelse:\n    test_size = 0.15\n    val_size = 0.15\n\n# First split: train+val vs test\ntrain_val_df, test_df = train_test_split(\n    combined_df,\n    test_size=test_size,\n    stratify=combined_df['dialect'],\n    random_state=42\n)\n\n# Second split: train vs val\nval_size_adjusted = val_size / (1 - test_size)\ntrain_df, val_df = train_test_split(\n    train_val_df,\n    test_size=val_size_adjusted,\n    stratify=train_val_df['dialect'],\n    random_state=42\n)\n\nprint(f\"\\nTrain set: {len(train_df)} samples\")\nprint(train_df['dialect'].value_counts())\nprint(f\"\\nValidation set: {len(val_df)} samples\")\nprint(val_df['dialect'].value_counts())\nprint(f\"\\nTest set: {len(test_df)} samples\")\nprint(test_df['dialect'].value_counts())\n\n# Visualize splits\nplot_split_comparison(train_df, val_df, test_df, 'viz_06_split_distribution.png')\n\n# ============================================================================\n# SAVE DATASETS\n# ============================================================================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"SAVING DATASETS\")\nprint(\"=\" * 80)\n\ntrain_df.to_csv(OUTPUT_DIR + 'cleaned_bangla_train.csv', index=False, encoding='utf-8')\nval_df.to_csv(OUTPUT_DIR + 'cleaned_bangla_val.csv', index=False, encoding='utf-8')\ntest_df.to_csv(OUTPUT_DIR + 'cleaned_bangla_test.csv', index=False, encoding='utf-8')\n\nprint(f\"✓ Saved: {OUTPUT_DIR}cleaned_bangla_train.csv\")\nprint(f\"✓ Saved: {OUTPUT_DIR}cleaned_bangla_val.csv\")\nprint(f\"✓ Saved: {OUTPUT_DIR}cleaned_bangla_test.csv\")\n\n# ============================================================================\n# SUMMARY STATISTICS\n# ============================================================================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"SUMMARY STATISTICS\")\nprint(\"=\" * 80)\n\ndef get_stats(df, name):\n    print(f\"\\n{name}:\")\n    print(f\"  Total samples: {len(df)}\")\n    print(f\"  Avg text length: {df['text'].str.len().mean():.2f} chars\")\n    print(f\"  Min text length: {df['text'].str.len().min()} chars\")\n    print(f\"  Max text length: {df['text'].str.len().max()} chars\")\n    print(f\"  Median text length: {df['text'].str.len().median():.2f} chars\")\n    \nget_stats(train_df, \"Training Set\")\nget_stats(val_df, \"Validation Set\")\nget_stats(test_df, \"Test Set\")\n\n# Create comprehensive summary table\nsummary_data = []\nfor split_name, split_df in [('Train', train_df), ('Validation', val_df), ('Test', test_df)]:\n    for dialect in DIALECTS:\n        dialect_data = split_df[split_df['dialect'] == dialect]\n        if len(dialect_data) > 0:\n            summary_data.append({\n                'Split': split_name,\n                'Dialect': dialect,\n                'Count': len(dialect_data),\n                'Avg Length': dialect_data['text'].str.len().mean(),\n                'Min Length': dialect_data['text'].str.len().min(),\n                'Max Length': dialect_data['text'].str.len().max()\n            })\n\nsummary_df = pd.DataFrame(summary_data)\nprint(\"\\n\" + \"=\" * 80)\nprint(\"DETAILED SUMMARY TABLE\")\nprint(\"=\" * 80)\nprint(summary_df.to_string(index=False))\n\n# Save summary\nsummary_df.to_csv(OUTPUT_DIR + 'dataset_summary.csv', index=False)\nprint(f\"\\n✓ Saved: {OUTPUT_DIR}dataset_summary.csv\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"PREPROCESSING COMPLETE!\")\nprint(\"=\" * 80)\n\n# Display samples\nprint(\"\\nSample from training set:\")\nprint(train_df.head(3).to_string())\nprint(\"\\nSample from validation set:\")\nprint(val_df.head(3).to_string())\nprint(\"\\nSample from test set:\")\nprint(test_df.head(3).to_string())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T13:19:39.393724Z","iopub.execute_input":"2025-12-16T13:19:39.394128Z","iopub.status.idle":"2025-12-16T13:19:54.128538Z","shell.execute_reply.started":"2025-12-16T13:19:39.394093Z","shell.execute_reply":"2025-12-16T13:19:54.127404Z"}},"outputs":[],"execution_count":null}]}